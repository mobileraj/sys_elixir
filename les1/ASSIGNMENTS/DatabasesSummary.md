Database Management Systems (DBMS) are used for a wide variety of applications. They are some of the most complex pieces of software and hardware, with some millions of lines of code. A Relational DBMS stores its data in tables, with columns representing fields and rows representing tuples of data. Many of the concepts/components described in this summary are applicable to all DBMS; however, it will focus primarily on relational DBMS (RDBMS).

Before relational databases were created, databases were poorly abstracted, coupling application code with the internal representation and structure of the database (which was highly subject to change). The relational model abstracts the database "guts" (indexes, formats, storage, structure, etc) uses the logical links between data. Thus, the DBMS has a wide range of flexibility in implementing different models and strategies for its implementation. In the relational model , each column can be represented by a set of possible values (a domain) and each table is a mathematical relation on those columns. The user interacts with this relation rather than with the files and raw data which form the relation. The relational model enables the use of a highly-expressive universal query language (SQL) based on formal logic. This is a brief snapshot of what a relational database is.

In an actual implementation, many complex mechanisms work together to effectively manage the database. In the first step of the life of a query, the client connects to the DBMS through the client communications manager, which facilitates sessions, requests, and responds. Queries are then passed to the process manager. The process manager creates, schedules, and manages the workers of the DBMS. A worker will use the relational query processor (RQP) to compile and execute its SQL query. The RQP calls the transactional storage manager. The transactional store manager obtains and manipulates data as specified by the query. From here, the stack unwinds. Data and computations bubble up the stack until the results of the query, polished and organized, are sent back to the client. This represents the standard life of a query; client sends a message, message is processed, and data is returned back to the client. 

The client communications manager is the first tier in an DBMS. This tier is responsible for 1) creating and maintaining sessions with clients; 2) receiving and delegating requests to the rest of the database; 3) responding to clients with query results. After the client communications manager creates a session with the client, the process model must create, schedule, and maintain a worker which is responsible for handling the clients queries. The DBMS must ultimately transfer data to and from the client. To do this, it employs two main buffers: the I/O buffer for the disk, and a communications buffer for queueing client data. The disk buffer writes to and reads from both the database and the query logs. The communications buffer is used when the client is fetching data in small chunks. The DBMS, anticipating the client's future requests of the data, loads it into this buffer. These buffers are necessary for data transfer and manipulation, from client to server to client.

Often, DBMS's are implemented around one of three main process models for creating workers. The first model creates an OS process for each worker. This model was often used in early DBMS’s due to the lack of efficient and effective support for OS threads. Due to the isolated nature of processes, process-based workers are relatively easy to implement, as many of the common problems and bugs in multithreading only arise when using shared environments and resources. However, OS processes are expensive to create. The second model uses a process pool to limit the number of processes. Processes are reused instead of re-created. Thus, this model tends to be far more resource efficient than the first model. Latency problems arise, however, when there are not enough processes in the pool to handle the incoming requests. The final model speeds things up by running workers in threads instead of processes. Threads (whether they are OS threads or special DBMS-implemented threads) are much cheaper to create than processes and tend to be faster. However, due to the fact that threads share memory, multithreading introduces many possible bugs to the system, such as race conditions. This makes such systems much harder to implement correctly. Each process model comes with its own pros and cons. Often, an DBMS will implement multiple strategies to provide the best performance and scalability.

There are three major parallelism models: the Shared-Memory, Shared-Disk, and Shared-Nothing models. In the Shared-Memory, multiple CPUs use the same RAM and Disk memory. The simplicity of this model is attractive; the DBMS as a whole behaves like a single multiprocessor device. All three process models (covered in the last paragraph) become easy to implement. Multiple independent SQL requests can be ran in parallel, or single complex queries can be be parallelized. Coordinating between CPUs is easy. However, if one device shuts down, the whole system will typically shut down as well, due to the interconnected nature of the CPUs. An alternative is the Shared-Disk model. Similarly to the previous, long-term disk memory is shared; RAM memory, however, is kept separate with each CPU. This model comes with the important benefit that the failure of one machine will not necessarily affect the others, and thus the DBMS can continue running in the event of a single CPU failure. However, coordination between devices can be difficult, as there is no shared RAM memory. Thus, systems with these models need two additional components: a distributed lock manager, and a protocol to manage the distributed, buffered caches. Shared-Memory and Shared-Disk models both come with the vulnerability to data corruption. In the third model, Shared-Nothing, a group of independent machines are used to run the DBMS. As the name implies, no memory is shared; data is communicated through message passing. Tables are stored piecewise throughout the various machines. A single query may will be run in part on all machines, the results of which are then aggregated. This allows for even load-balancing. Overall, this model tends to be highly scalable, as each processor can be considered its own machine. However, there are two main problems: complex cross-processor coordination is required, as they do not have shared memory. In addition, although one machine's failure may not affect directly affect the others, part of the database will become unavailable (given the partitioning scheme described). Thus, redundancy is required for reliability. As with the three process models, commercial DBMSs will typically use a hybrid of all three of these models, given their pros and cons. 

The relational query processor (RQP) is responsible for validating, optimizing, and executing SQL queries. The RQP has four main components: the SQL parser, the query rewriter, the query optimizer, and the query executor. When the RQP receives SQL, the first step is the parse it. Table and column names/aliases are resolved, authentication is verified (which includes ensuring that no constraints are violated; i.e. the database is in a consistent state), and the original SQL is converted into an internal representation of the query. Next, the query (or, more likely, the internal representation of the query) is rewritten to make query optimization easier. The main (and original) function of this phase is to expand logical views into their corresponding real tables in the database. In addition, expressions are simplified (constant arithmetic, always-false conditions, etc) or expanded (e.g., to include implicit predicates). The query is flattened into SELECT-FROM-WHERE blocks; this makes query optimization much simpler, as the optimizer does not have to untangle deeply nested queries. 

Once the query is rewritten, the optimizer turns that query into a plan/dataflow. This dataflow can be represented as an expression tree of low-level database operators. To create this tree of operators, the query optimizer uses many complex algorithms and heuristics to achieve the desired query in the most efficient manner possible. The optimizer transforms the declarative query into a tree of imperative commands, to be traversed by the query executor. The query executor will typically use a model based on iterators. Each iterator corresponds to a node. Each iterator stores 1) its input iterators; 2) a method for generating the next resulting tuple based on the tuples generated by its inputs. Both output and input tuples are 1) stored either in a buffer pool (reference counted), or in a memory heap; and 2) referenced by the iterators using a "tuple descriptor", which typically contains either the physical address of the tuple or the primary key of a row in the database.  To generate the next resulting tuple, the root iterator must recursively generate the tuple from all its input iterators. Generating the next tuple of an iterator can be represented as a typical procedure on the call stack. The plan at a high-level describes the data flow, and iterators follow this plan in the control flow. Those four components describe how SQL is turned into query results.

There are two basic models for long-term storage management: 1) the DBMS interacts directly with low level device drivers; 2) the DBMS uses the OS file system. The first model comes with the benefits of speed and fine-grained control, since the DBMS can read and write memory directly without meddling with the file system. The main problem with this model is that it is not cross-OS, as different operating systems come with different procedures for directly accessing memory. The second model delegates this responsibility to the file system (which has cross-OS interfaces) by creating one big file to store the database. With modern OS implementations, this strategy is decently fast (although still slower than the first model). In addition, most OS's will store the file in roughly contiguous memory, and so under this model we have approximate control of the location of our data. Most modern databases used the second model, due to its ease, interface, and relative speed. However, the filesystem model comes with an additional two main sets of problems to consider: correctness and performance. The DBMS must have full control of timing and ordering disk writes to ensure atomicity. Second, the OS optimizations for file access are not suited for database tasks. The OS tends to optimize for physical access (i.e. close in time and space) to the file. However, the DBMS must optimize logically, based on the read requests from the client. These are not easily accessible to the OS, as only the compiled database commands (from the RQP) are exposed to the OS. Thus, the DBMS must employ a custom buffer, as the OS buffer is largely useless for optimization. Modern OS's provide event hooks to intercept disk I/O events, which can be used to avoid the problem of having two active buffers. 

The transactional storage manager, at the final step of a query's journey, is responsible for managing disk IO and ensuring that transactions are atomic, isolated, durable, and serializable. Atomicity is a "do or do not do" mentality; either a transaction happens completely, or it does not happen at all. Different transactions (even ones that use the same tables) should be isolated, meaning that they do not see each other's writes. Each transaction should be able to pretend it is the only one operating in the database. Durability means that if a transaction makes a change to the database, that change is persisted, even if the system crashes. The DBMS must ensure that concurrent transactions are at least partial serializable. Serializability means that operations executing concurrently should behave as if they were executed in some sequence. Serializability and isolation are closely tied; if transactions are isolated, then they are serializable and vice versa.  To accomplish these goals, the transactional storage manage employs four tightly coupled components: 1) the buffer pool; 2) access methods; 3) the lock manage (or another method to achieve serializability); and 4) the log manager. 

The "short-term memory" of the DBMS, the buffer is typically implemented as a large, shared pool of memory stored as an array of frames, or blocks of memory which are the same size as disk blocks. Thus, data can be copied between the buffer and the disk without the overhead of reformatting or reshaping the data. Along with this array, the buffer contains a hash table which links pages in the buffer pool to real pages in the database so as to "remember" where modified data should be written to. The transactional storage manager defines many access methods to interface with the data structures (such as heaps and trees) used internally by the database. These must be implemented to behave correctly in settings where recoverability and high concurrency are necessary. 

There are three main methods to achieve serializability: 1) multi version concurrency control (MVCC); 2) optimistic concurrency control (OCC); and 3) two-phase locking (2PL). In MVCC, each transaction is guaranteed a consistent view of the database state sometime in the past. Typically, to be efficiently implemented, it does not guarantee full serializability. It uses one of two weaker semantics: SNAPSHOT ISOLATION or READ CONSISTENCY. In SNAPSHOT ISOLATION, each worker gets a timestamped "snapshot" of the database at the beginning of a transaction. The transaction only commits if no other writes have happened after the timestamp. READ CONSISTENCY is very similar, except that each SQL *statement* (instead of transaction) gets a different timestamped snapshot of the database. Thus, more transactions can commit; however, different statements within a transaction will have different views on the database. In OCC, there is no blocking. Transactions maintain a history of reads and writes. When a transaction is about to commit it checks for conflicts and rollbacks as needed. This is optimal for environments where there are few conflicts as there is no need to add the overhead of managing locks. Two-phase locking (where the lock is grabbed, operation performed, and then released) stores a shared read lock and a write lock for each resource. A lock is just a name used to represent a resource managed by the DBMS. Acquires and releases of a lock must be atomic, thus (generally) a transaction must be removed in the same operation that the lock is removed. To support locking and unlocking, two hash tables are used: 1) a lock table, which stores the lock name as a key mapped to the waiting transactions; and 2) a transaction table which stores the transaction id mapped to the thread state and lock requests. 2PL is likely the most widely used method of serializability. The other methods, if used at all, tend to be supplemental to 2PL.

2PL systems can be used to implement full serializability; however, for speed (at the cost of correctness), sometimes weaker levels of isolation are used. Other than full serializability, there are four main isolation levels: READ UNCOMMITTED, READ COMMITTED, REPEATABLE READ, and CURSOR STABILITY. In READ UNCOMMITTED, transactions may read any version of the data. No read locks are used. Slightly stronger, in READ COMMITTED transactions may read a committed version of the data (thus repeated reads may obtain different snapshots of the data). Read requests acquire a read lock before accessing, but release the lock immediately after. REPEATABLE READ is similar to READ COMMITTED, except that the lock is only released at the end of the transaction. At the cost of some speed, this ensures that reading the same data multiple times returns the same copy. CURSOR STABILITY is weaker than REPEATABLE READ but stronger than READ COMMITTED. In this isolation level, the tuple which is about to being fetched (and is being manipulated on) is locked. This prevents overwrites in typical "read-compute-write" flows (where the value is read, manipulated, and then written).

In addition to locks, most DBMSs use latches. Latches are a less expensive supplement to locks. They are more primitive and stored directly in memory with their resource rather than in a table. Latches are used to manage access to internal database structures, while locks are generally used for rows and tables. Latches are acquired by specialized code (within the DBMS internals); locks acquisition is driven by data access. 

The log manager has two main jobs: 1) manage rollback of aborted transactions (which ensures atomicity); and 2) recover from abnormal shutdowns (which ensures durability). To accomplish these jobs it stores a "history", or persistent sequence of logs. There are two types of logs: physical and logical ("physiological"). Physical logs store the before and after images of the database rows. This is convenient and easy to redo/undo. However, this tends to take a lot of space. For operations with a logical inverse, logs are created which just record the operation done. This uses much less space, however, it requires more time and complexity to undo and redo such operations. In practice, a combination of the two is used.

The most commonly used protocol for logging is the write-ahead logging (WAL) protocol. In this protocol, there are several rules. First, logs must be flushed before a database page is. This ensures atomicity as incomplete transactions can then be undone based on the logs. Second, Logs must be flushed in order; otherwise else, the logs cannot be "replayed" to recreate the database state after a crash. Third, a log must be flushed before the transaction can be reported to have committed successfully. Along with the second rule, this ensures durability, so that actions can be undone. 

The log manager must guarantee a "fast-path" for committed transactions, along with efficient rollback for aborted transactions and quick recovery from abnormal shutdown. One common mode for efficient storage management is the "DIRECT, STEAL/NOT-FORCE" mode. DIRECT means that data objects are updated in place. STEAL means that dirty buffer pages (i.e. ones with uncommitted changes) can be flushed to the database without concern for transactional correctness. NOT-FORCE means that a committed buffer page does not need to be flushed before the commit request returns successfully to the user. This mode still follows the WAL protocol. It optimizes for the fast-path by giving the buffer manager and the disk scheduler enough flexibility to decide on the memory management and I/O policies which will be most effective. It is very scalable.  However, it adds the additional burden on the log manager to efficiently handle undoing stolen pages and redoing changes to not-forced pages of committed transactions. In practice, it tends to lead to major performance benefits, as these cases do not commonly occur. Often, it is combined with the "DIRECT, NOT-STEAL/NOT-FORCE" mode, where buffer pages are not stolen unless there are no clean pages to flush, in which case it degrades back to the "DIRECT, STEAL/NOT-FORCE" mode. 

The log manager is also responsible for crash recovery. To ensure changes that the user made are persisted, it must be able to successfully restore the database state after a system shutdown. The naive method for doing this would be to redo all logs up until the current point. Not all logs need to be redone, however. The DBMS can compute the sequence number of the last log that needs to be redone; this is known as the recovery log sequence number (recovery LSN). In practice, this recovery LSN is computed at "checkpoints", each with at least enough information to initiate the log analysis process for rebooting the DBMS.

An index is a physical storage structure for quickly accessing data and enforcing uniqueness constraints. There are a variety of ways to implement indexes, one of which is the B+-tree. A B+-tree is a perfectly balanced tree which stores nodes in blocks, with a pointers between each node potentially pointing to another block. When a block becomes full, it is split into sub-blocks. Data is stored in the leaf nodes. Database application developers can't directly observe or manipulate these structures; it is managed by complex transactional schemes. Because of this, logging and locking indexes comes with some special rules. Logging for indexes (as well as other physical structures) does not always need to be done. A block split in an index is invisible to the user; it does not need to be "undone" (only redone in the case of a crash). This improves log and recovery efficiency (although the code becomes more complex). For locking indexes, there are three main schemes: conservation, latch-coupling, and right-link schemes. In conservation schemes, two concurrent transactions can only run together if they will not conflict in their use of content (where one transaction's changes could affect another). This tends to be overly restrictive, however. An alternative is a latch-coupling scheme. When traversing the tree, the current node is always latched, and the next node is latched before visiting. This strategy is only applicable to B+-trees, and thus will not work with different types of trees for indexes. The final scheme is the right link scheme. Each nodes stores a link to its right hand neighbor. Only the current node is latched; when a node detects a split, it "moves right" to find the new correct location. This scheme works with trees other than the B+-tree and requires less latching. Both right link and latch-coupling schemes are fairly common in databases.

One common problem that arises in systems that support both full serializability and tuple-level locking is the phantom problem. When querying an index, only the tuples obtained form the query are locked, as locking the entire index would be very expensive. However, when only the tuples are locked, an insert from another query might create a tuple in between other tuples selected by the predicate. Thus, repeated reads with the same predicate can obtain additional results. Thus, some level of "logical locking" (locking the entries that satisfy a given predicate) is needed. Unfortunately, directly locking by arbitrary predicates is also too expensive, as this would require the ability to determine compare predicates for overlap between logical spaces (rather than physical areas, which are easy to check for overlap). To solve this problem, something known as next-key locking is typically employed. In this setup inserts must grab an exclusive (write) lock to the tuple with the next key in the index. Thus, it becomes impossible to insert between two tuples that are locked, as to do so would require locking the second one. To prevent inserts at the tail end of a logical region, the first tuple outside of the region is locked with a shared read lock. Thus, a physical object (the tuple) can be used as a substitute for logical locking.

There are five major shared components that reoccur in databases: 1) the catalog manger; 2) memory context allocators; 3) disk management subsystems; 4) replication services; and 5) administration services. The catalog manager stores table metadata (columns, schemas, keys, etc). It is stored in table form to make the data more compact and simpler to use as the metadata is stored and accessed the same way that as the actual data. The parser and optimizer may materialize catalog data as necessary for usage. The next component is memory allocation. The database uses lots of memory allocation, both for query compilation/optimization and for query execution. Instead of garbage collection, a DBMS will typically used memory contexts, which are in-memory data structures which each store a list of contiguous memory regions. Whenever a component needs memory, it will create a memory context. The component will control the where the memory is created and when it is released. This greatly reduces overhead, as the component can handle the memory as is ideal for its usage. 

The third shared component is the disk management subsystems. Disks vary widely in their storage capacity and data bandwidth, and thus access to them needs to be managed. Nowadays, complex systems that have a disk-like interface (like RAID or SAN) are commonly used in place of disks. These systems present many challenges and failure points that normal disks do not have, presenting more challenges for management subsystems. Despite their challenges, these systems are typically used due to their easy installation, easy management, and high capacity. The fourth component is the replication service. Databases provide a physically separate backup for extra reliability, in case of say or a fire or a complete failure of the primary database. There are three main schemes for replication: physical, where the entire database is duplicated every period; trigger-based, where change records are sent to a replication table, periodically shipped to a remote site to perform modifications; and log-based, where a log sniffer process intercepts log writes and delivers them to a remote system. Physical replication tends to be far too expensive. Trigger-based adds an additional performance penalty which may not be bearable in certain workloads. Thus, log-based is the only scheme which is good enough for high-end settings, as it comes with low overhead and virtually no performance penalty. The final shared component is the administration tooling. This includes statistics gathering, database exportation, bulk load operations, physical restructuring/reorganizing, and resource governance (for database admin). These tools are efficiently implemented to run while the database is in use, as some applications need the database 24/7.

Databases are a well-researched and highly developed field of computer science. Lessons from this research applies to many data-intensive fields.






